---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alerts-alertmanager
  labels:
    prometheus.io/instance: main
    instance.prometheus.io/main: enable
    instance.prometheus.io/prompp: enable
spec:
  groups:
  - name: alerts-alertmanager
    rules:
    - alert: AlertmanagerFailedReload
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod}}:
          configuration has failed to load
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload
        summary: Reloading an Alertmanager configuration has failed.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(alertmanager_config_last_reload_successful[5m]) == 0
      for: 10m
      labels:
        severity: critical
    # - alert: AlertmanagerMembersInconsistent
    #   annotations:
    #     description: >-
    #       {{ $labels.namespace }}/{{ $labels.pod}}:
    #       Alertmanager has only found {{ $value }} members of the {{ $labels.job }} cluster.
    #     runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent
    #     summary: A member of an Alertmanager cluster has not found all other cluster members.
    #   expr: |-
    #     # Without max_over_time, failed scrapes could create false negatives, see
    #     # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
    #       max_over_time(alertmanager_cluster_members[5m])
    #     < on (namespace,service,cluster) group_left
    #       count by(namespace,service,cluster) (max_over_time(alertmanager_cluster_members[5m]))
    #   for: 15m
    #   labels:
    #     severity: critical
    - alert: AlertmanagerFailedToSendAlerts
      annotations:
        description: >-
          {{ $labels.namespace }}/{{ $labels.pod}}:
          {{ $labels.integration }}:
          {{ $labels.reason }}:
          {{ $value | humanizePercentage }}
        summary: An Alertmanager instance failed to send more than 1% of notifications
      expr: |-
        (
          sum without(reason) (rate(alertmanager_notifications_failed_total[5m]))
        /
          (rate(alertmanager_notifications_total[5m]) != 0)
        ) > 0.01
      for: 5m
      labels:
        severity: warning
    - alert: AlertmanagerClusterFailedToSendAlerts
      annotations:
        description: >-
          {{ $labels.namespace }}:
          {{ $labels.integration }}:
          {{ $value | humanizePercentage }}
        summary: All Alertmanager instances in a cluster failed to send notifications to integration.
      expr: |-
        min without(instance, pod) (
          sum without(reason) (rate(alertmanager_notifications_failed_total[5m]))
        /
          (rate(alertmanager_notifications_total[5m]) != 0)
        )
        > 0.01
      for: 5m
      labels:
        severity: warning
    - alert: AlertmanagerConfigInconsistent
      annotations:
        description: >-
          {{ $labels.namespace }}:
          {{ $labels.exported_cluster }}:
          {{ $value }} configurations
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent
        summary: Alertmanager instances within the same cluster have different configurations.
        # TODO figure this out
        # I don't have an alertmanager cluster right now
      expr: |-
        count_values without(config_hash, instance, pod) ("config_hash", alertmanager_config_hash) != 1
      for: 20m
      labels:
        severity: critical
    - alert: AlertmanagerClusterDown
      annotations:
        description: >-
          {{ $labels.namespace }}:
          {{ $labels.exported_cluster }}:
          {{ $value | humanizePercentage }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown
        summary: Half or more of the Alertmanager instances within the same cluster are down.
      expr: |-
        (
          count without(instance, pod) (
            avg_over_time(up{job="alertmanager"}[5m]) < 0.5
          )
        /
          count without(instance, pod) (
            up{job="alertmanager"}
          )
        )
        >= 0.5
      for: 5m
      labels:
        severity: critical
    - alert: AlertmanagerClusterCrashlooping
      annotations:
        description: >-
          {{ $labels.namespace }}:
          {{ $labels.exported_cluster }}:
          {{ $value | humanizePercentage }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping
        summary: Half or more of the Alertmanager instances within the same cluster are crashlooping.
      expr: |-
        (
          count by(namespace,service,cluster) (
            changes(process_start_time_seconds{job="alertmanager"}[10m]) > 4
          )
        /
          count by(namespace,service,cluster) (
            up{job="alertmanager"}
          )
        )
        >= 0.5
      for: 5m
      labels:
        severity: critical
